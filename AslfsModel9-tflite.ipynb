{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecc9bfa0",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-08-25T20:11:58.814072Z",
     "iopub.status.busy": "2023-08-25T20:11:58.813684Z",
     "iopub.status.idle": "2023-08-25T20:12:08.855497Z",
     "shell.execute_reply": "2023-08-25T20:12:08.854278Z"
    },
    "papermill": {
     "duration": 10.052535,
     "end_time": "2023-08-25T20:12:08.858500",
     "exception": false,
     "start_time": "2023-08-25T20:11:58.805965",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n",
      "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n",
      "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras.mixed_precision as mixed_precision\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_addons as tfa\n",
    "import gc\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea59e556",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-25T20:12:08.872904Z",
     "iopub.status.busy": "2023-08-25T20:12:08.871560Z",
     "iopub.status.idle": "2023-08-25T20:12:09.189492Z",
     "shell.execute_reply": "2023-08-25T20:12:09.188233Z"
    },
    "papermill": {
     "duration": 0.327762,
     "end_time": "2023-08-25T20:12:09.192135",
     "exception": false,
     "start_time": "2023-08-25T20:12:08.864373",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "seed_everything()\n",
    "\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect(tpu=\"local\") # \"local\" for 1VM TPU\n",
    "    strategy = tf.distribute.TPUStrategy(tpu)\n",
    "    print(\"on TPU\")\n",
    "    N_REPLICAS = strategy.num_replicas_in_sync\n",
    "    print(\"REPLICAS: \", N_REPLICAS)\n",
    "    \n",
    "except:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    N_REPLICAS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96b5a5aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-25T20:12:09.207772Z",
     "iopub.status.busy": "2023-08-25T20:12:09.207343Z",
     "iopub.status.idle": "2023-08-25T20:12:09.222813Z",
     "shell.execute_reply": "2023-08-25T20:12:09.221426Z"
    },
    "papermill": {
     "duration": 0.026791,
     "end_time": "2023-08-25T20:12:09.225409",
     "exception": false,
     "start_time": "2023-08-25T20:12:09.198618",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_subset = None # Size of subset of data to use, for faster debugging. Set None for all data.\n",
    "# device='CPU' This is automated now\n",
    "val_fold = 0 # Choose from 0th-5th fold as the validation\n",
    "model_output_name = 'modelv9'  #Specify the name of the output model\n",
    "verbose = 2  # (0): No output. (1): progress bar. (2): One line per epoch (Recommended for saving a new version)\n",
    "\n",
    "# parameters of model architecture\n",
    "num_heads = 2      # Number of attention heads\n",
    "head_size = 128    # Dimension of each head\n",
    "kernel_size = 4   # Kernel size of local attention\n",
    "DIM = num_heads*head_size  # This will be the dimension of the embedding and attention\n",
    "CNN_ksize = int(np.sqrt(DIM))\n",
    "dropout = 0.33 \n",
    "num_enc_layers = 6    # Number of attention blocks for input and output \n",
    "num_dec_layers = 4\n",
    "# Parameters for training\n",
    "epochs = 200\n",
    "lr = 3e-4*N_REPLICAS\n",
    "lr_min = 1e-6 # minimum learning rate over time\n",
    "warmup_epochs = 5\n",
    "resume_epoch = 0 # Suppose we want to resume training\n",
    "decay_epochs = 200 #scale at which the lr decays. \n",
    "decay_type = 'exponential'\n",
    "awp_start_epoch = 20\n",
    "batch_size = 32*N_REPLICAS\n",
    "early_stop_patience = 10  # If the validation error doesn't improve in this many epochs, stop early\n",
    "#dropout_start_epoch = 10\n",
    "#Parameters of data preprocessing and augmentation\n",
    "# Set to none for no augmentation\n",
    "rot_deg = (-20., 20.) \n",
    "shift = (-0.1, 0.1)\n",
    "scale = (0.8, 1.2)\n",
    "shear = (-0.2, 0.2)\n",
    "#time_dilation = (0.8, 1.2)\n",
    "time_mask_prob = 0.02\n",
    "\n",
    "# Parameters of structure of data\n",
    "NUM_POINTS = 42\n",
    "CHANNELS = NUM_POINTS*6\n",
    "MAX_INPUT_LEN = 600 #Because we drop nan frames\n",
    "MAX_OUTPUT_LEN = 31+2 #+2 for the start and end token \n",
    "\n",
    "f = open('/kaggle/input/asl-fingerspelling/character_to_prediction_index.json')\n",
    "char2id = json.load(f)\n",
    "f.close()\n",
    "start_token = '\\t'\n",
    "start_token_idx = 59\n",
    "end_token = '\\n'\n",
    "end_token_idx = 60\n",
    "pad_token = 'P'\n",
    "TKPAD = 61 \n",
    "char2id[start_token] = start_token_idx   #Use as start token\n",
    "char2id[end_token]= end_token_idx   #Use as end token\n",
    "char2id[pad_token] = TKPAD\n",
    "id2char = {char2id[c]:c for c in char2id.keys()}\n",
    "VOCAB_SIZE = len(char2id) #includes start, end, pad tokens.\n",
    "LMPAD = -100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7ccfc65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-25T20:12:09.239098Z",
     "iopub.status.busy": "2023-08-25T20:12:09.238685Z",
     "iopub.status.idle": "2023-08-25T20:12:09.244961Z",
     "shell.execute_reply": "2023-08-25T20:12:09.244111Z"
    },
    "papermill": {
     "duration": 0.015744,
     "end_time": "2023-08-25T20:12:09.247136",
     "exception": false,
     "start_time": "2023-08-25T20:12:09.231392",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_files = [f'/kaggle/input/aslfstfrecords3/fold{i}.tfrecords' for i in range(6)]\n",
    "val_files = train_files[val_fold]\n",
    "del train_files[val_fold]\n",
    "\n",
    "num_samples_per_file = [11155, 11230, 11141, 11237, 11267, 11178]\n",
    "total_num_samples = sum(num_samples_per_file)\n",
    "val_num_samples = num_samples_per_file[val_fold]\n",
    "train_num_samples = total_num_samples - val_num_samples\n",
    "steps_per_epoch = int(np.ceil(train_num_samples/batch_size))\n",
    "val_steps = int(np.ceil(val_num_samples/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "093f9ccb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-25T20:12:09.261719Z",
     "iopub.status.busy": "2023-08-25T20:12:09.260759Z",
     "iopub.status.idle": "2023-08-25T20:12:09.301611Z",
     "shell.execute_reply": "2023-08-25T20:12:09.300576Z"
    },
    "papermill": {
     "duration": 0.051091,
     "end_time": "2023-08-25T20:12:09.304093",
     "exception": false,
     "start_time": "2023-08-25T20:12:09.253002",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ECA(tf.keras.layers.Layer):\n",
    "    def __init__(self, kernel_size=5, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv = tf.keras.layers.Conv1D(1, kernel_size=kernel_size, strides=1, padding=\"same\", use_bias=False)\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        nn = tf.keras.layers.GlobalAveragePooling1D()(inputs, mask=mask)\n",
    "        nn = tf.expand_dims(nn, -1)\n",
    "        nn = self.conv(nn)\n",
    "        nn = tf.squeeze(nn, -1)\n",
    "        nn = tf.nn.sigmoid(nn)\n",
    "        nn = nn[:,None,:]\n",
    "        return inputs * nn\n",
    "\n",
    "class CausalDWConv1D(tf.keras.layers.Layer):\n",
    "    def __init__(self, \n",
    "        kernel_size=CNN_ksize,\n",
    "        dilation_rate=1,\n",
    "        use_bias=False,\n",
    "        depthwise_initializer='glorot_uniform',\n",
    "        name='', **kwargs):\n",
    "        super().__init__(name=name,**kwargs)\n",
    "        self.causal_pad = tf.keras.layers.ZeroPadding1D((dilation_rate*(kernel_size-1),0),name=name + '_pad')\n",
    "        self.dw_conv = tf.keras.layers.DepthwiseConv1D(\n",
    "                            kernel_size,\n",
    "                            strides=1,\n",
    "                            dilation_rate=dilation_rate,\n",
    "                            padding='valid',\n",
    "                            use_bias=use_bias,\n",
    "                            depthwise_initializer=depthwise_initializer,\n",
    "                            name=name + '_dwconv')\n",
    "        self.supports_masking = True\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.causal_pad(inputs)\n",
    "        x = self.dw_conv(x)\n",
    "        return x\n",
    "\n",
    "class Conv1DBlock(layers.Layer):\n",
    "    def __init__(self,channel_size=DIM,kernel_size=CNN_ksize, dilation_rate=1, drop_rate=dropout,\n",
    "          expand_ratio=2, se_ratio=0.25, activation='swish', **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.expand = layers.Dense(channel_size*expand_ratio)\n",
    "        self.dwconv = CausalDWConv1D(kernel_size,dilation_rate=dilation_rate,use_bias=False)\n",
    "        self.bn = layers.BatchNormalization(momentum=0.95)\n",
    "        self.eca = ECA()\n",
    "        self.proj = layers.Dense(channel_size,use_bias=True)\n",
    "        self.drop = layers.Dropout(drop_rate, noise_shape=(None,1,1))\n",
    "        self.add = layers.Add()\n",
    "        self.supports_masking=True\n",
    "    '''\n",
    "    efficient conv1d block, @hoyso48\n",
    "    '''\n",
    "    # Expansion phase\n",
    "    def call(self, x):\n",
    "        bypass = x\n",
    "        x = self.expand(x)\n",
    "        # Depthwise Convolution\n",
    "        x = self.dwconv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.eca(x)\n",
    "        x = self.proj(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.add([x,bypass])\n",
    "        return x   \n",
    "\n",
    "class TokenEmbedding(layers.Layer):\n",
    "    \"\"\"Input: (*, seq_len). Output: (*, seq_len, embedding_dim)\n",
    "    Input 0 will be masked.\n",
    "    CHECK: Can we speed up by making pos a constant tensor?\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=VOCAB_SIZE, max_len=MAX_OUTPUT_LEN, embed_dim=DIM, \n",
    "                 pad_value=TKPAD, dropout=dropout, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.tok_emb = layers.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_emb = layers.Embedding(max_len, embed_dim)\n",
    "        self.drop = layers.Dropout(dropout)\n",
    "        self.pad = pad_value\n",
    "        self.scale = tf.math.sqrt(tf.cast(embed_dim,tf.float32))\n",
    "        self.pos = tf.range(max_len)[None,:]\n",
    "        self.ln = layers.LayerNormalization(center=False)\n",
    "        \n",
    "    def call(self, x):\n",
    "        #B = tf.shape(x)[0]\n",
    "        T = tf.shape(x)[1]\n",
    "        x = self.tok_emb(x)\n",
    "        x *= self.scale # Set the relative scale of the token and position embeddng\n",
    "        pos = self.pos_emb(self.pos[:,:T])\n",
    "        x = x+pos\n",
    "        x = self.drop(x)\n",
    "        x = self.ln(x)\n",
    "        return x\n",
    "    \n",
    "    def compute_mask(self,x, mask=None):\n",
    "        new_mask = x!=self.pad\n",
    "        if mask:\n",
    "            new_mask = new_mask & mask\n",
    "        return new_mask\n",
    "    \n",
    "\n",
    "class CoordEmbedding(layers.Layer):\n",
    "    \"\"\"Input: (*, seq_len, channels). Output: (*, seq_len, embedding_dim)\n",
    "    Also creates mask for any time frame that includes mask_value\"\"\"\n",
    "    \n",
    "    def __init__(self, max_len=MAX_INPUT_LEN, embed_dim=DIM, dropout=dropout, mask_value=LMPAD, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.masking = layers.Masking(mask_value=mask_value)\n",
    "        self.emb = layers.Dense(embed_dim) #bias or not is debatable\n",
    "        self.bn = layers.BatchNormalization(momentum=0.95)\n",
    "        self.convs = [Conv1DBlock(embed_dim,kernel_size=CNN_ksize,drop_rate=dropout) for _ in range(3)]\n",
    "        \n",
    "        self.pos_emb = layers.Embedding(max_len, embed_dim)\n",
    "        self.drop = layers.Dropout(dropout)\n",
    "        self.scale = tf.math.sqrt(tf.cast(embed_dim,tf.float32))\n",
    "        self.pad = mask_value\n",
    "        self.pos = tf.range(max_len)[None,:]\n",
    "        self.ln = layers.LayerNormalization(center=False)\n",
    "        \n",
    "    def call(self, x):\n",
    "        #B = tf.shape(x)[0]\n",
    "        T = tf.shape(x)[1]\n",
    "        x = self.masking(x)\n",
    "        x = self.emb(x)\n",
    "        x = self.bn(x)\n",
    "        \n",
    "        for conv in self.convs:\n",
    "            x = conv(x)\n",
    "        \n",
    "        x *= self.scale  # Set the relative scale of the token and position embeddng\n",
    "        pos = self.pos_emb(self.pos[:,:T])\n",
    "        x = x + pos\n",
    "        x = self.drop(x)\n",
    "        x = self.ln(x)\n",
    "        return x\n",
    "    \n",
    "    def compute_mask(self, x, mask=None):\n",
    "        new_mask = tf.reduce_all(x!=self.pad, axis=-1)\n",
    "        if mask:\n",
    "            new_mask = new_mask & mask\n",
    "        return new_mask\n",
    "    \n",
    "class MLP(layers.Layer):\n",
    "    \"\"\"input and output are the same shape. \n",
    "    Dense layer to 4x the hidden dim and then dense layer back to 1x hidden dim\"\"\"\n",
    "    def __init__(self, input_dim=DIM, dropout=dropout, bias=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.c_fc = layers.Dense(4*input_dim, use_bias=bias, activation='gelu')\n",
    "        self.c_proj = layers.Dense(input_dim, use_bias=bias)\n",
    "        self.drop = layers.Dropout(dropout) # CHECK noise_shape later\n",
    "        self.supports_masking=True\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "class EncoderTransformerBlock(layers.Layer):\n",
    "    def __init__(self,head_size=head_size, num_heads=num_heads, dropout=dropout, kernel_size=kernel_size, num_local=2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_local = num_local\n",
    "        self.lns = [layers.LayerNormalization(center=False) for _ in range(self.num_local)]\n",
    "        self.adds = [layers.Add() for _ in range(self.num_local)]\n",
    "        self.local_atts = [layers.MultiHeadAttention(num_heads,head_size,dropout=dropout,use_bias=False) for _ in range(self.num_local)]\n",
    "        self.local_mask = tf.cast(tf.linalg.band_part(\n",
    "                        tf.ones((MAX_INPUT_LEN,MAX_INPUT_LEN)), kernel_size-1, 0), tf.bool)\n",
    "        \n",
    "        self.ln2 = layers.LayerNormalization(center=False)\n",
    "        self.add2 = layers.Add()\n",
    "        self.global_att = layers.MultiHeadAttention(num_heads,head_size,dropout=dropout,use_bias=False)\n",
    "        \n",
    "        self.ln3 = layers.LayerNormalization(center=False)\n",
    "        self.mlp = MLP(head_size*num_heads)\n",
    "        self.add3 = layers.Add()\n",
    "        \n",
    "        self.support_masking=True   \n",
    "    \n",
    "    def call(self,x):\n",
    "        T = tf.shape(x)[1]\n",
    "        for i in range(self.num_local):\n",
    "            bypass = x\n",
    "            x = self.lns[i](x)\n",
    "            x = self.local_atts[i](x,x, attention_mask = self.local_mask[:T,:T])\n",
    "            x = self.adds[i]([x,bypass])\n",
    "        \n",
    "        bypass = x\n",
    "        x = self.ln2(x)\n",
    "        x = self.global_att(x,x)\n",
    "        x = self.add3([x,bypass])                \n",
    "        \n",
    "        bypass = x\n",
    "        x = self.ln3(x)\n",
    "        x = self.mlp(x)\n",
    "        x = self.add3([x,bypass])\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderTransformerBlock(layers.Layer):\n",
    "    \"\"\"\"\"\"\n",
    "    def __init__(self,head_size=head_size, num_heads=num_heads, dropout=dropout, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.support_masking=True\n",
    "        self.ln1 = layers.LayerNormalization(center=False)\n",
    "        self.ln2 = layers.LayerNormalization(center=False)\n",
    "        self.ln3 = layers.LayerNormalization(center=False)\n",
    "        self.self_att = layers.MultiHeadAttention(num_heads,head_size,dropout=dropout,use_bias=False)\n",
    "        self.cross_att = layers.MultiHeadAttention(num_heads,head_size,dropout=dropout,use_bias=False)\n",
    "        self.mlp = MLP(head_size*num_heads)\n",
    "        self.add1 = layers.Add()\n",
    "        self.add2 = layers.Add()\n",
    "        self.add3 = layers.Add()\n",
    "        \n",
    "    def call(self, x, y):\n",
    "        # x is the source sequence (landmarks), y is the target seuqence (phrase)\n",
    "        bypass = y\n",
    "        y = self.ln1(y)\n",
    "        y = self.self_att(y,y,use_causal_mask=True) # Check that self att works out with the double masks\n",
    "        y = self.add1([y,bypass])\n",
    "              \n",
    "        bypass = y \n",
    "        y = self.ln2(y)\n",
    "        y = self.cross_att(y,x) # Check that cross att works out with the mask of x.\n",
    "        y = self.add2([y, bypass])\n",
    "        \n",
    "        bypass= y \n",
    "        y = self.ln3(y)\n",
    "        y = self.mlp(y)\n",
    "        y = self.add3([y,bypass])\n",
    "        \n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a763a96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-25T20:12:09.319467Z",
     "iopub.status.busy": "2023-08-25T20:12:09.319038Z",
     "iopub.status.idle": "2023-08-25T20:12:09.349837Z",
     "shell.execute_reply": "2023-08-25T20:12:09.348601Z"
    },
    "papermill": {
     "duration": 0.042539,
     "end_time": "2023-08-25T20:12:09.352380",
     "exception": false,
     "start_time": "2023-08-25T20:12:09.309841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class OneCycleLR(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"Unified single-cycle learning rate scheduler for tensorflow.\n",
    "    2022 Hoyeol Sohn <hoeyol0730@gmail.com>\"\"\"\n",
    "    def __init__(self, lr=1e-4, epochs=10, steps_per_epoch=100, steps_per_update=1,\n",
    "                resume_epoch=0, decay_epochs=10, sustain_epochs=0, warmup_epochs=0,\n",
    "                lr_start=0, lr_min=0, warmup_type='linear', decay_type='cosine', **kwargs):\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "        self.lr = float(lr)\n",
    "        self.epochs = float(epochs)\n",
    "        self.steps_per_update = float(steps_per_update)\n",
    "        self.resume_step = float(resume_epoch*steps_per_epoch)\n",
    "        self.steps_per_epoch = float(steps_per_epoch)\n",
    "        self.decay_steps = float(decay_epochs*steps_per_epoch)\n",
    "        self.sustain_steps = float(sustain_epochs*steps_per_epoch)\n",
    "        self.warmup_steps = float(warmup_epochs*steps_per_epoch)\n",
    "        self.total_steps = float(epochs*steps_per_epoch)\n",
    "        self.lr_start = float(lr_start)\n",
    "        self.lr_min = float(lr_min)\n",
    "        self.decay_type = decay_type\n",
    "        self.warmup_type = warmup_type\n",
    "        \n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        if self.resume_step:\n",
    "            step = step + self.resume_step\n",
    "\n",
    "        step = tf.cond(step > self.decay_steps, lambda :self.decay_steps, lambda :step)\n",
    "        step = tf.math.truediv(step, self.steps_per_update) * self.steps_per_update\n",
    "\n",
    "        warmup_cond = step < self.warmup_steps\n",
    "        decay_cond = step >= (self.warmup_steps + self.sustain_steps)\n",
    "        \n",
    "        if self.warmup_type == 'linear':\n",
    "            lr = tf.cond(warmup_cond, lambda: tf.math.divide_no_nan(self.lr-self.lr_start , self.warmup_steps) * step + self.lr_start, lambda: self.lr)\n",
    "        elif self.warmup_type == 'exponential':\n",
    "            factor = tf.pow(self.lr_start, 1/self.warmup_steps)\n",
    "            lr = tf.cond(warmup_cond, lambda: (self.lr - self.lr_start) * factor**(self.warmup_steps - step) + self.lr_start, lambda: self.lr)\n",
    "        elif self.warmup_type == 'cosine':\n",
    "            lr = tf.cond(warmup_cond, lambda: 0.5 * (self.lr - self.lr_start) * (1 + tf.cos(3.14159265359 * (self.warmup_steps - step)  / self.warmup_steps)) + self.lr_start, lambda:self.lr)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "                    \n",
    "        if self.decay_type == 'linear':\n",
    "            lr = tf.cond(decay_cond, lambda: self.lr + (self.lr_min-self.lr)/(self.decay_steps - self.warmup_steps - self.sustain_steps)*(step - self.warmup_steps - self.sustain_steps), lambda:lr)\n",
    "        elif self.decay_type == 'exponential':\n",
    "            factor = tf.pow(self.lr_min, 1/(self.decay_steps - self.warmup_steps - self.sustain_steps))\n",
    "            lr = tf.cond(decay_cond, lambda: (self.lr - self.lr_min) * factor**(step - self.warmup_steps - self.sustain_steps) + self.lr_min, lambda:lr)\n",
    "        elif self.decay_type == 'cosine':\n",
    "            lr = tf.cond(decay_cond, lambda: 0.5 * (self.lr - self.lr_min) * (1 + tf.cos(3.14159265359 * (step - self.warmup_steps - self.sustain_steps) / (self.decay_steps - self.warmup_steps - self.sustain_steps))) + self.lr_min, lambda:lr)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return lr\n",
    "\n",
    "    def plot(self):\n",
    "        step = max(1, int(self.total_steps)//1000) #1 for total_steps < 1000, total_steps//1000 else\n",
    "        eps = list(range(0,int(self.total_steps),step))\n",
    "        learning_rates = [self(x) for x in eps]\n",
    "        eps = [x/self.steps_per_epoch for x in eps]\n",
    "        plt.scatter(eps,learning_rates,2)\n",
    "        plt.title('Learning rate schedule')\n",
    "        plt.xlabel('epochs')\n",
    "        plt.ylabel('learning rate')\n",
    "        plt.show()    \n",
    "\n",
    "class AWP(tf.keras.Model):\n",
    "    \"\"\"2022 Hoyeol Sohn <hoeyol0730@gmail.com>\"\"\"\n",
    "    def __init__(self, *args, delta=0.1, eps=1e-4, start_step=0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.delta = delta\n",
    "        self.eps = eps\n",
    "        self.start_step = start_step\n",
    "        \n",
    "    def train_step_awp(self, data):\n",
    "        # Unpack the data. Its structure depends on your model and\n",
    "        # on what you pass to `fit()`.\n",
    "        x, y = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)\n",
    "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "        params = self.trainable_variables\n",
    "        params_gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        for i in range(len(params_gradients)):\n",
    "            grad = tf.zeros_like(params[i]) + params_gradients[i]\n",
    "            delta = tf.math.divide_no_nan(self.delta * grad , tf.math.sqrt(tf.reduce_sum(grad**2)) + self.eps)\n",
    "            self.trainable_variables[i].assign_add(delta)\n",
    "        with tf.GradientTape() as tape2:\n",
    "            y_pred = self(x, training=True)\n",
    "            new_loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "            if hasattr(self.optimizer, 'get_scaled_loss'):\n",
    "                new_loss = self.optimizer.get_scaled_loss(new_loss)\n",
    "            \n",
    "        gradients = tape2.gradient(new_loss, self.trainable_variables)\n",
    "        if hasattr(self.optimizer, 'get_unscaled_gradients'):\n",
    "            gradients =  self.optimizer.get_unscaled_gradients(gradients)\n",
    "        for i in range(len(params_gradients)):\n",
    "            grad = tf.zeros_like(params[i]) + params_gradients[i]\n",
    "            delta = tf.math.divide_no_nan(self.delta * grad , tf.math.sqrt(tf.reduce_sum(grad**2)) + self.eps)\n",
    "            self.trainable_variables[i].assign_sub(delta)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        # self_loss.update_state(loss)\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def train_step(self, data):\n",
    "        return tf.cond(self._train_counter < self.start_step, lambda:super(AWP, self).train_step(data), lambda:self.train_step_awp(data))        \n",
    "        \n",
    "def masked_loss(label, pred):\n",
    "    mask = label != TKPAD\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "    loss = loss_object(label, pred)\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "    return loss\n",
    "\n",
    "def masked_accuracy(label, pred):\n",
    "    pred = tf.argmax(pred, axis=2)\n",
    "    label = tf.cast(label, pred.dtype)\n",
    "    match = label == pred\n",
    "    mask = label != TKPAD\n",
    "    match = match & mask\n",
    "    match = tf.cast(match, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(match)/tf.reduce_sum(mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df18afb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-25T20:12:09.366729Z",
     "iopub.status.busy": "2023-08-25T20:12:09.366011Z",
     "iopub.status.idle": "2023-08-25T20:12:18.133652Z",
     "shell.execute_reply": "2023-08-25T20:12:18.132375Z"
    },
    "papermill": {
     "duration": 8.777823,
     "end_time": "2023-08-25T20:12:18.136039",
     "exception": false,
     "start_time": "2023-08-25T20:12:09.358216",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 600, 252)]   0           []                               \n",
      "                                                                                                  \n",
      " coord_embedding (CoordEmbeddin  (None, 600, 256)    1039119     ['input_1[0][0]']                \n",
      " g)                                                                                               \n",
      "                                                                                                  \n",
      " encoder_transformer_block (Enc  (None, 600, 256)    1311744     ['coord_embedding[0][0]']        \n",
      " oderTransformerBlock)                                                                            \n",
      "                                                                                                  \n",
      " encoder_transformer_block_1 (E  (None, 600, 256)    1311744     ['encoder_transformer_block[0][0]\n",
      " ncoderTransformerBlock)                                         ']                               \n",
      "                                                                                                  \n",
      " encoder_transformer_block_2 (E  (None, 600, 256)    1311744     ['encoder_transformer_block_1[0][\n",
      " ncoderTransformerBlock)                                         0]']                             \n",
      "                                                                                                  \n",
      " encoder_transformer_block_3 (E  (None, 600, 256)    1311744     ['encoder_transformer_block_2[0][\n",
      " ncoderTransformerBlock)                                         0]']                             \n",
      "                                                                                                  \n",
      " encoder_transformer_block_4 (E  (None, 600, 256)    1311744     ['encoder_transformer_block_3[0][\n",
      " ncoderTransformerBlock)                                         0]']                             \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 32)]         0           []                               \n",
      "                                                                                                  \n",
      " encoder_transformer_block_5 (E  (None, 600, 256)    1311744     ['encoder_transformer_block_4[0][\n",
      " ncoderTransformerBlock)                                         0]']                             \n",
      "                                                                                                  \n",
      " token_embedding (TokenEmbeddin  (None, 32, 256)     24576       ['input_2[0][0]']                \n",
      " g)                                                                                               \n",
      "                                                                                                  \n",
      " decoder_transformer_block (Dec  (None, 32, 256)     1049344     ['encoder_transformer_block_5[0][\n",
      " oderTransformerBlock)                                           0]',                             \n",
      "                                                                  'token_embedding[0][0]']        \n",
      "                                                                                                  \n",
      " decoder_transformer_block_1 (D  (None, 32, 256)     1049344     ['encoder_transformer_block_5[0][\n",
      " ecoderTransformerBlock)                                         0]',                             \n",
      "                                                                  'decoder_transformer_block[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " decoder_transformer_block_2 (D  (None, 32, 256)     1049344     ['encoder_transformer_block_5[0][\n",
      " ecoderTransformerBlock)                                         0]',                             \n",
      "                                                                  'decoder_transformer_block_1[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " decoder_transformer_block_3 (D  (None, 32, 256)     1049344     ['encoder_transformer_block_5[0][\n",
      " ecoderTransformerBlock)                                         0]',                             \n",
      "                                                                  'decoder_transformer_block_2[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " dense_27 (Dense)               (None, 32, 62)       15934       ['decoder_transformer_block_3[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 13,147,469\n",
      "Trainable params: 13,143,885\n",
      "Non-trainable params: 3,584\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7b81dcb425c0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "tf.config.optimizer.set_jit(True)\n",
    "\n",
    "inp1 = layers.Input((MAX_INPUT_LEN,6*NUM_POINTS))\n",
    "inp2 = layers.Input((MAX_OUTPUT_LEN-1,))\n",
    "x = CoordEmbedding()(inp1)\n",
    "y = TokenEmbedding()(inp2)\n",
    "for _ in range(num_enc_layers):\n",
    "    x = EncoderTransformerBlock(head_size,num_heads,dropout, kernel_size)(x)\n",
    "for _ in range(num_dec_layers):\n",
    "    y = DecoderTransformerBlock(head_size, num_heads, dropout)(x,y)\n",
    "logits = layers.Dense(VOCAB_SIZE)(y)\n",
    "model = tf.keras.Model((inp1,inp2),logits) \n",
    "\n",
    "# Define learning rate and weight decay schedule\n",
    "schedule = OneCycleLR(lr, epochs, warmup_epochs=warmup_epochs, steps_per_epoch=steps_per_epoch,\n",
    "                  resume_epoch=resume_epoch, decay_epochs=decay_epochs, lr_min=lr_min,\n",
    "                  decay_type=decay_type, warmup_type='linear')\n",
    "decay_schedule = OneCycleLR(0.05*lr, epochs, warmup_epochs=warmup_epochs, \n",
    "                        steps_per_epoch=steps_per_epoch,\n",
    "                        resume_epoch=resume_epoch, decay_epochs=decay_epochs, \n",
    "                        lr_min=0.05*lr_min, decay_type=decay_type, warmup_type='linear')\n",
    "# Define optimizers\n",
    "optimizer=tfa.optimizers.RectifiedAdam(learning_rate=schedule,weight_decay=decay_schedule,sma_threshold=4)\n",
    "optimizer=tfa.optimizers.Lookahead(optimizer,sync_period=5)\n",
    "\n",
    "model.compile(loss = masked_loss,\n",
    "             optimizer = optimizer,\n",
    "             metrics=[masked_accuracy],\n",
    "             steps_per_execution=steps_per_epoch\n",
    "             )\n",
    "    \n",
    "model.summary()\n",
    "model.load_weights('/kaggle/input/aslfsmodel9/modelv9.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7692cb8c",
   "metadata": {
    "papermill": {
     "duration": 0.009237,
     "end_time": "2023-08-25T20:12:18.154767",
     "exception": false,
     "start_time": "2023-08-25T20:12:18.145530",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Model loaded\n",
    "# Create TFLite model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dfdb516",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-25T20:12:18.177267Z",
     "iopub.status.busy": "2023-08-25T20:12:18.176702Z",
     "iopub.status.idle": "2023-08-25T20:12:18.183148Z",
     "shell.execute_reply": "2023-08-25T20:12:18.181995Z"
    },
    "papermill": {
     "duration": 0.02114,
     "end_time": "2023-08-25T20:12:18.185441",
     "exception": false,
     "start_time": "2023-08-25T20:12:18.164301",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = [f'x_right_hand_{i}' for i in range(21)] + [f'x_left_hand_{i}' for i in range(21)] \n",
    "Y = [f'y_right_hand_{i}' for i in range(21)] + [f'y_left_hand_{i}' for i in range(21)]\n",
    "FEATURE_COLUMNS = X + Y \n",
    "NUM_POINTS=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "306625a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-25T20:12:18.206874Z",
     "iopub.status.busy": "2023-08-25T20:12:18.206349Z",
     "iopub.status.idle": "2023-08-25T20:12:18.220205Z",
     "shell.execute_reply": "2023-08-25T20:12:18.218465Z"
    },
    "papermill": {
     "duration": 0.027784,
     "end_time": "2023-08-25T20:12:18.222581",
     "exception": false,
     "start_time": "2023-08-25T20:12:18.194797",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tf_nan_mean(x, axis=0, keepdims=False):\n",
    "    return tf.reduce_sum(tf.where(tf.math.is_nan(x), tf.zeros_like(x), x), axis=axis, keepdims=keepdims) / tf.reduce_sum(tf.where(tf.math.is_nan(x), tf.zeros_like(x), tf.ones_like(x)), axis=axis, keepdims=keepdims)\n",
    "\n",
    "def tf_nan_std(x, center=None, axis=0, keepdims=False):\n",
    "    if center is None:\n",
    "        center = tf_nan_mean(x, axis=axis,  keepdims=True)\n",
    "    d = x - center\n",
    "    return tf.math.sqrt(tf_nan_mean(d * d, axis=axis, keepdims=keepdims))\n",
    "\n",
    "@tf.function()\n",
    "def preprocess4inference(x):\n",
    "    # Input is of shape (frames, x_of_points+y_of_points)\n",
    "    \n",
    "    x = tf.stack([x[:,:NUM_POINTS], x[:,NUM_POINTS:]], axis = 2)\n",
    "    \n",
    "    # Normalize the coordinates\n",
    "    x = x-tf_nan_mean(x,axis=(0,1))\n",
    "    x = x/tf_nan_std(x,axis=(0,1,2))\n",
    "    \n",
    "    x = tf.reshape(x,(-1,NUM_POINTS*2))\n",
    "    \n",
    "    dx = tf.cond(tf.shape(x)[0]>1,\n",
    "                 lambda:tf.pad(x[1:,:] - x[:-1,:], [[0,1],[0,0]]),\n",
    "                 lambda:tf.zeros_like(x))   \n",
    "    dx2 = tf.cond(tf.shape(x)[0]>2,\n",
    "             lambda:tf.pad(x[2:,:] - x[:-2,:], [[0,2],[0,0]]),\n",
    "             lambda:tf.zeros_like(x))\n",
    "    \n",
    "    x = tf.concat([x, dx, dx2], axis=1)\n",
    "    \n",
    "    # Drop the nan frames\n",
    "    x = tf.boolean_mask(x, ~tf.reduce_all(tf.math.is_nan(x),axis=1))\n",
    "    x = tf.where(tf.math.is_nan(x), 0., x)\n",
    "       \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91e00f8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-25T20:12:18.243878Z",
     "iopub.status.busy": "2023-08-25T20:12:18.243461Z",
     "iopub.status.idle": "2023-08-25T20:12:18.411933Z",
     "shell.execute_reply": "2023-08-25T20:12:18.411063Z"
    },
    "papermill": {
     "duration": 0.181833,
     "end_time": "2023-08-25T20:12:18.414263",
     "exception": false,
     "start_time": "2023-08-25T20:12:18.232430",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>file_id</th>\n",
       "      <th>sequence_id</th>\n",
       "      <th>participant_id</th>\n",
       "      <th>phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_landmarks/5414471.parquet</td>\n",
       "      <td>5414471</td>\n",
       "      <td>1816796431</td>\n",
       "      <td>217</td>\n",
       "      <td>3 creekhouse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_landmarks/5414471.parquet</td>\n",
       "      <td>5414471</td>\n",
       "      <td>1816825349</td>\n",
       "      <td>107</td>\n",
       "      <td>scales/kuhaylah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_landmarks/5414471.parquet</td>\n",
       "      <td>5414471</td>\n",
       "      <td>1816909464</td>\n",
       "      <td>1</td>\n",
       "      <td>1383 william lanier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_landmarks/5414471.parquet</td>\n",
       "      <td>5414471</td>\n",
       "      <td>1816967051</td>\n",
       "      <td>63</td>\n",
       "      <td>988 franklin lane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_landmarks/5414471.parquet</td>\n",
       "      <td>5414471</td>\n",
       "      <td>1817123330</td>\n",
       "      <td>89</td>\n",
       "      <td>6920 northeast 661st road</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              path  file_id  sequence_id  participant_id  \\\n",
       "0  train_landmarks/5414471.parquet  5414471   1816796431             217   \n",
       "1  train_landmarks/5414471.parquet  5414471   1816825349             107   \n",
       "2  train_landmarks/5414471.parquet  5414471   1816909464               1   \n",
       "3  train_landmarks/5414471.parquet  5414471   1816967051              63   \n",
       "4  train_landmarks/5414471.parquet  5414471   1817123330              89   \n",
       "\n",
       "                      phrase  \n",
       "0               3 creekhouse  \n",
       "1            scales/kuhaylah  \n",
       "2        1383 william lanier  \n",
       "3          988 franklin lane  \n",
       "4  6920 northeast 661st road  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('/kaggle/input/asl-fingerspelling/train.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91803151",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-25T20:12:18.435358Z",
     "iopub.status.busy": "2023-08-25T20:12:18.434928Z",
     "iopub.status.idle": "2023-08-25T20:12:19.034580Z",
     "shell.execute_reply": "2023-08-25T20:12:19.033300Z"
    },
    "papermill": {
     "duration": 0.613249,
     "end_time": "2023-08-25T20:12:19.037170",
     "exception": false,
     "start_time": "2023-08-25T20:12:18.423921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched video for phrase: 3 creekhouse\n"
     ]
    }
   ],
   "source": [
    "def get_video_from_sid(sid):\n",
    "    row = train_df[train_df['sequence_id'] == sid]\n",
    "    path = row['path'].iloc[0]\n",
    "    phrase = row['phrase'].iloc[0]\n",
    "    print('Fetched video for phrase:', phrase)\n",
    "    video = pd.read_parquet('/kaggle/input/asl-fingerspelling/'+path, \n",
    "                        columns=FEATURE_COLUMNS, \n",
    "                        filters=[('sequence_id','=',sid)])\n",
    "    return video\n",
    "\n",
    "video = get_video_from_sid(1816796431)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a353bfcb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-25T20:12:19.059042Z",
     "iopub.status.busy": "2023-08-25T20:12:19.058657Z",
     "iopub.status.idle": "2023-08-25T20:13:27.597680Z",
     "shell.execute_reply": "2023-08-25T20:13:27.596653Z"
    },
    "papermill": {
     "duration": 68.562719,
     "end_time": "2023-08-25T20:13:27.610025",
     "exception": false,
     "start_time": "2023-08-25T20:12:19.047306",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 59)\n",
      "tf.Tensor([18  0 34 49 36 36 42 39 46 52 50 36], shape=(12,), dtype=int64)\n",
      "3 creekhouse\n"
     ]
    }
   ],
   "source": [
    "class TFLiteModel(tf.Module):\n",
    "    # This will only process one video at a time\n",
    "    def __init__(self, model):\n",
    "        super(TFLiteModel, self).__init__()\n",
    "        self.model = model\n",
    "        self.thresh = 7.03\n",
    "\n",
    "    def translate(self,lm):\n",
    "        seq2 = tf.cast(tf.ones((1,1))*59, tf.int32) #Start tokens\n",
    "        #all_logits = tf.constant([[0]*59+[100]+[0]*2], tf.float32)\n",
    "        x = self.model.get_layer(index=1)(lm,training=False)\n",
    "        for i in [2,3,4,5,6,8]:\n",
    "            x = self.model.get_layer(index=i)(x, training=False)\n",
    "        ended = False\n",
    "        for _ in range(MAX_OUTPUT_LEN-1):\n",
    "            if ended:\n",
    "                pred = tf.constant(61,tf.int32)[tf.newaxis]\n",
    "                #logits = tf.constant([[0]*61+[100]], tf.float32) \n",
    "            else:\n",
    "                y = self.model.get_layer(index=9)(seq2,training=False)\n",
    "                for i in [10,11,12,13]:\n",
    "                    y = self.model.get_layer(index=i)(x,y,training=False)\n",
    "                logits = self.model.get_layer(index=14)(y,training=False)\n",
    "                logits = logits[:,-1,:]\n",
    "                pred = tf.argmax(logits,axis=-1,output_type=tf.int32)\n",
    "                if pred == 60:\n",
    "                    ended = True\n",
    "            seq2 = tf.concat([seq2,pred[:,None]],axis=1)\n",
    "            #all_logits = tf.concat([all_logits,logits],axis=0)\n",
    "        return seq2#, all_logits        \n",
    "    \n",
    "    @tf.function(input_signature=[tf.TensorSpec(shape=[None, len(FEATURE_COLUMNS)], dtype=tf.float32, name='inputs')])\n",
    "    def __call__(self, inputs, training=False):\n",
    "        # Preprocess Data\n",
    "        x = tf.cast(inputs, tf.float32)\n",
    "        x = x[None]\n",
    "        x = tf.cond(tf.shape(x)[1] == 0, lambda: tf.zeros((1, 1, len(FEATURE_COLUMNS))), lambda: tf.identity(x))\n",
    "        x = x[0]\n",
    "        x = preprocess4inference(x)\n",
    "        x = x[None]\n",
    "        x = self.translate(x)\n",
    "        x = x[0]\n",
    "        end_idx = tf.argmax(tf.cast(tf.equal(x, 60), tf.int32))\n",
    "        end_idx = tf.where(tf.math.less(end_idx, 1), tf.constant(MAX_OUTPUT_LEN, dtype=tf.int64), end_idx)\n",
    "        x = x[1:end_idx]\n",
    "        #logits = logits[1:end_idx]\n",
    "        #x = tf.boolean_mask(x, tf.math.reduce_std(logits,axis=-1)>self.thresh)\n",
    "        x = tf.one_hot(x, 59)\n",
    "        return {'outputs': x}\n",
    "    \n",
    "tflitemodel_base = TFLiteModel(model)\n",
    "sample_pred = tflitemodel_base(video)[\"outputs\"]\n",
    "print(sample_pred.shape)\n",
    "print(tf.argmax(sample_pred,axis=-1))\n",
    "print(''.join([id2char[i] for i in tf.argmax(sample_pred,axis=-1).numpy()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fe4a9a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-25T20:13:27.632001Z",
     "iopub.status.busy": "2023-08-25T20:13:27.630984Z",
     "iopub.status.idle": "2023-08-25T20:17:06.081753Z",
     "shell.execute_reply": "2023-08-25T20:17:06.080310Z"
    },
    "papermill": {
     "duration": 218.464833,
     "end_time": "2023-08-25T20:17:06.084683",
     "exception": false,
     "start_time": "2023-08-25T20:13:27.619850",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "keras_model_converter = tf.lite.TFLiteConverter.from_keras_model(tflitemodel_base) \n",
    "keras_model_converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n",
    "keras_model_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "keras_model_converter.target_spec.supported_types = [tf.float16]\n",
    "tflite_model = keras_model_converter.convert()\n",
    "with open('/kaggle/working/model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "    \n",
    "infargs = {\"selected_columns\" : FEATURE_COLUMNS}\n",
    "\n",
    "with open('inference_args.json', \"w\") as json_file:\n",
    "    json.dump(infargs, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "615648ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-25T20:17:06.107138Z",
     "iopub.status.busy": "2023-08-25T20:17:06.106725Z",
     "iopub.status.idle": "2023-08-25T20:17:08.990265Z",
     "shell.execute_reply": "2023-08-25T20:17:08.988699Z"
    },
    "papermill": {
     "duration": 2.898121,
     "end_time": "2023-08-25T20:17:08.993387",
     "exception": false,
     "start_time": "2023-08-25T20:17:06.095266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: model.tflite (deflated 20%)\r\n",
      "  adding: inference_args.json (deflated 85%)\r\n"
     ]
    }
   ],
   "source": [
    "!zip submission.zip './model.tflite' './inference_args.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df4e2cf1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-25T20:17:09.016737Z",
     "iopub.status.busy": "2023-08-25T20:17:09.016315Z",
     "iopub.status.idle": "2023-08-25T20:17:09.516676Z",
     "shell.execute_reply": "2023-08-25T20:17:09.515223Z"
    },
    "papermill": {
     "duration": 0.515634,
     "end_time": "2023-08-25T20:17:09.519573",
     "exception": false,
     "start_time": "2023-08-25T20:17:09.003939",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 creekhouse\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(\"/kaggle/working/model.tflite\")\n",
    "\n",
    "REQUIRED_SIGNATURE = \"serving_default\"\n",
    "REQUIRED_OUTPUT = \"outputs\"\n",
    "\n",
    "with open (\"/kaggle/input/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n",
    "    character_map = json.load(f)\n",
    "rev_character_map = {j:i for i,j in character_map.items()}\n",
    "\n",
    "found_signatures = list(interpreter.get_signature_list().keys())\n",
    "\n",
    "if REQUIRED_SIGNATURE not in found_signatures:\n",
    "    raise KernelEvalException('Required input signature not found.')\n",
    "\n",
    "prediction_fn = interpreter.get_signature_runner(\"serving_default\")\n",
    "output = prediction_fn(inputs=video)\n",
    "prediction_str = \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(output[REQUIRED_OUTPUT], axis=1)])\n",
    "print(prediction_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59c35ce6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-25T20:17:09.541730Z",
     "iopub.status.busy": "2023-08-25T20:17:09.541309Z",
     "iopub.status.idle": "2023-08-25T20:17:09.901581Z",
     "shell.execute_reply": "2023-08-25T20:17:09.900170Z"
    },
    "papermill": {
     "duration": 0.374269,
     "end_time": "2023-08-25T20:17:09.903983",
     "exception": false,
     "start_time": "2023-08-25T20:17:09.529714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction for nan_video\n",
      "+44-50-25-47-57-57\n",
      "prediction for empty video\n",
      "+44-50-25-47-57-57\n"
     ]
    }
   ],
   "source": [
    "# Check that it works on empty and nan videos\n",
    "\n",
    "nan_video = np.array([[np.nan]*len(FEATURE_COLUMNS)],dtype=np.float32)\n",
    "empty_video = np.array([[]],dtype=np.float32)\n",
    "\n",
    "print('prediction for nan_video')\n",
    "print(''.join([rev_character_map.get(i,\"\") for i in \n",
    "               np.argmax(prediction_fn(inputs=nan_video)[REQUIRED_OUTPUT],axis=1)\n",
    "              ]))\n",
    "print('prediction for empty video')\n",
    "print(''.join([rev_character_map.get(i,\"\") for i in \n",
    "               np.argmax(prediction_fn(inputs=empty_video)[REQUIRED_OUTPUT],axis=1)\n",
    "              ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f93204be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-25T20:17:09.925533Z",
     "iopub.status.busy": "2023-08-25T20:17:09.925109Z",
     "iopub.status.idle": "2023-08-25T20:17:52.893782Z",
     "shell.execute_reply": "2023-08-25T20:17:52.892259Z"
    },
    "papermill": {
     "duration": 42.982475,
     "end_time": "2023-08-25T20:17:52.896199",
     "exception": false,
     "start_time": "2023-08-25T20:17:09.913724",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched video for phrase: 3 creekhouse\n",
      "Prediction: 3 creekhouse\n",
      "Fetched video for phrase: scales/kuhaylah\n",
      "Prediction: sales/puhayla\n",
      "Fetched video for phrase: 1383 william lanier\n",
      "Prediction: 1383 william lanier\n",
      "Fetched video for phrase: 988 franklin lane\n",
      "Prediction: 988 franklin lane\n",
      "Fetched video for phrase: 6920 northeast 661st road\n",
      "Prediction: 6920 northeast 61st road\n",
      "Fetched video for phrase: www.freem.ne.jp\n",
      "Prediction: www.freem.me.jp\n",
      "Fetched video for phrase: https://jsi.is/hukuoka\n",
      "Prediction: https://jsi.is/hthkuoka\n",
      "Fetched video for phrase: 239613 stolze street\n",
      "Prediction: 239613 stolze street\n",
      "Fetched video for phrase: 242-197-6202\n",
      "Prediction: +261-71-74-60-20-28\n",
      "Fetched video for phrase: 271097 bayshore boulevard\n",
      "Prediction: 271097 bayshore boulevard\n",
      "Fetched video for phrase: federico pearson\n",
      "Prediction: federico pearson\n",
      "Fetched video for phrase: /carpina/hope_&_faith/litle\n",
      "Prediction: /carpina/hope_&_fith/litle\n",
      "Fetched video for phrase: dine-in/code/\n",
      "Prediction: dine-in/code/\n",
      "Fetched video for phrase: +264-97-568-217-145\n",
      "Prediction: +264-97-5468-227-45\n",
      "Fetched video for phrase: +51-2721-208-63\n",
      "Prediction: +51-27121-208-63\n",
      "Fetched video for phrase: wildberries_ru\n",
      "Prediction: wilders_rus_\n",
      "Fetched video for phrase: leona owens\n",
      "Prediction: leona owens\n",
      "Fetched video for phrase: projecteur-led\n",
      "Prediction: edura eurre\n",
      "Fetched video for phrase: +220-557-859-04\n",
      "Prediction: +220-557-859-04\n",
      "Fetched video for phrase: kati castro\n",
      "Prediction: kati castro\n",
      "Fetched video for phrase: 5566 hellertown road\n",
      "Prediction: +44-56-60-00\n",
      "Fetched video for phrase: 6867 granville drive\n",
      "Prediction: 6867 granville drive\n",
      "Fetched video for phrase: 1600 fire water\n",
      "Prediction: 1600 fit water\n",
      "Fetched video for phrase: +45-39-007-1887\n",
      "Prediction: +45-39-007-1887\n",
      "Fetched video for phrase: 65634/tennessee%20river\n",
      "Prediction: 65634/tennessee%20river\n",
      "Fetched video for phrase: 596-033-4046\n",
      "Prediction: 530-404-4848\n",
      "Fetched video for phrase: 18 cutter ridge road\n",
      "Prediction: 18 cutter ridge road\n",
      "Fetched video for phrase: tampa fl\n",
      "Prediction: tampa fl\n",
      "Fetched video for phrase: 492288 west 28th terrace south\n",
      "Prediction: 492288 west 28th terrace south\n",
      "Fetched video for phrase: 166 water power\n",
      "Prediction: www.winews.com/\n",
      "Fetched video for phrase: jami mcfarland\n",
      "Prediction: jami mcfarland\n",
      "Fetched video for phrase: www.horseillustrated.com/\n",
      "Prediction: www.horeseilustrated.com/\n",
      "Fetched video for phrase: 1121 east 2609th road\n",
      "Prediction: 1121 east 2609th road\n",
      "Fetched video for phrase: 1786 cll cinturon\n",
      "Prediction: 786 locin court\n",
      "Fetched video for phrase: 620-510-6135\n",
      "Prediction: 620-510-6135\n",
      "Fetched video for phrase: angeli/bombillas-a-presion\n",
      "Prediction: angeli/bombillas-a-presion\n",
      "Fetched video for phrase: 69 alec roy south road\n",
      "Prediction: 2660 comptony stoutheast road\n",
      "Fetched video for phrase: www.line2life.ru/\n",
      "Prediction: www.fanchina.com/\n",
      "Fetched video for phrase: 3719 oakview terrace drive\n",
      "Prediction: 3719 oakview terrace drive\n",
      "Fetched video for phrase: 6738 co 5670\n",
      "Prediction: 738 c 5670\n",
      "Fetched video for phrase: +677-256-09-109-015-266\n",
      "Prediction: +677-256-09-015-266\n",
      "Fetched video for phrase: 201 galveston country club dr\n",
      "Prediction: 201 glveston country court\n",
      "Fetched video for phrase: www.theorienttreasures.com\n",
      "Prediction: www.theorienttreasures.com\n",
      "Fetched video for phrase: shop.oliphan.com/tana-lea\n",
      "Prediction: shop.oliphan.com/tana-lea\n",
      "Fetched video for phrase: 9345 happy dirt\n",
      "Prediction: 9345 payler trail\n",
      "Fetched video for phrase: 6060 hubbards ferry road\n",
      "Prediction: 6060 hubbards ferry road\n",
      "Fetched video for phrase: 5126 whitewind drive\n",
      "Prediction: 5126 whitewind drive\n",
      "Fetched video for phrase: +39-39-09-657-04-19\n",
      "Prediction: +39-39-09-652-04-19\n",
      "Fetched video for phrase: 909-220-8829\n",
      "Prediction: +44-00-87-51\n",
      "Fetched video for phrase: 893-125-1663\n",
      "Prediction: 893-251-1633\n",
      "Fetched video for phrase: 4821 holly ridge circle\n",
      "Prediction: 4821 holly ridge circle\n",
      "Fetched video for phrase: 106-703-4104\n",
      "Prediction: anton walter\n",
      "Fetched video for phrase: alaina cross\n",
      "Prediction: asanton salison\n",
      "Fetched video for phrase: tomeka salinas\n",
      "Prediction: tomeka salinas\n",
      "Fetched video for phrase: acth\n",
      "Prediction: https://www.hangmart.com\n",
      "Fetched video for phrase: m-advice.co.jp/boosela\n",
      "Prediction: www.allering.com/\n",
      "Fetched video for phrase: roksanajagusz_/sh\n",
      "Prediction: roksanajagusz_1/sh\n",
      "Fetched video for phrase: 735-018-3603\n",
      "Prediction: 735-018-3603\n",
      "Fetched video for phrase: 536 west summer dawn drive\n",
      "Prediction: 536 west summer drive\n",
      "Fetched video for phrase: 6169 valley view parks\n",
      "Prediction: 655 west simmeria street\n",
      "Fetched video for phrase: gemalde-tiere\n",
      "Prediction: gemalde-tiere\n",
      "Fetched video for phrase: https://www.px139.com/\n",
      "Prediction: https://www.pl3f.com/\n",
      "Fetched video for phrase: neobychnye-muzei-mira/3585/\n",
      "Prediction: neobychnye-muzei-mira/3585/\n",
      "Fetched video for phrase: /temple_detail\n",
      "Prediction: /temple_detail\n",
      "Fetched video for phrase: www.ipsah.edu.my\n",
      "Prediction: www.ipsah.edu.my\n",
      "Fetched video for phrase: 957 pomeroy lake\n",
      "Prediction: 957 pomeroy lake\n",
      "Fetched video for phrase: 6676 district 15 road\n",
      "Prediction: 87 salico road\n",
      "Fetched video for phrase: mio-footwear/refugee\n",
      "Prediction: impa-werrfurger.com\n",
      "Fetched video for phrase: 299-571-6098\n",
      "Prediction: 299-571-6089\n",
      "Fetched video for phrase: mcdonald-s/iohk_charles\n",
      "Prediction: mondond-s/orles\n",
      "Fetched video for phrase: 8254 haywood smith\n",
      "Prediction: 386254 hawsone\n",
      "Fetched video for phrase: 820-455-2154\n",
      "Prediction: 820-455-2154\n",
      "Fetched video for phrase: 9700 suffolk hollow\n",
      "Prediction: 9700 lake hollow\n",
      "Fetched video for phrase: 897-183-4471\n",
      "Prediction: 897-183-4471\n",
      "Fetched video for phrase: 304-159-4008\n",
      "Prediction: 304-004-4000\n",
      "Fetched video for phrase: 568-200-8703\n",
      "Prediction: 568-200-8703\n",
      "Fetched video for phrase: www.tamtamtorino.it/ganaron\n",
      "Prediction: www.tamtamtorino.it/ganaron\n",
      "Fetched video for phrase: 861 twp road 5961\n",
      "Prediction: 861 twp road 5961\n",
      "Fetched video for phrase: 1088 west aberdeen street\n",
      "Prediction: 1088 west aberdeen street\n",
      "Fetched video for phrase: tempemarketplace.com\n",
      "Prediction: teppemarketplace.com\n",
      "Fetched video for phrase: https://www.cultifort.com\n",
      "Prediction: https://www.cultifort.com\n",
      "Fetched video for phrase: polcathejourney\n",
      "Prediction: olcathejourney\n",
      "Fetched video for phrase: 7648 denio\n",
      "Prediction: 7648 denio\n",
      "Fetched video for phrase: 49918/ideyoanimal/mebelrust/\n",
      "Prediction: 4991 eyoan joane mele\n",
      "Fetched video for phrase: 5270 brasher drive\n",
      "Prediction: 50 west 1st st\n",
      "Fetched video for phrase: 927 ernest surrency\n",
      "Prediction: 927 ernest surrency\n",
      "Fetched video for phrase: webthesis.biblio.polito.it\n",
      "Prediction: webthesis.biblio.polito.it\n",
      "Fetched video for phrase: 416019 woody brook\n",
      "Prediction: 416019 woody brook\n",
      "Fetched video for phrase: +20-4703-88149\n",
      "Prediction: +20-4703-88149\n",
      "Fetched video for phrase: fysiotherapiedewaard.nl/585685\n",
      "Prediction: fysiotherapiedewaard.nl/578568\n",
      "Fetched video for phrase: 23-tillbehor/\n",
      "Prediction: tabaha barrett\n",
      "Fetched video for phrase: 4156 south 141st street\n",
      "Prediction: 4156 south 141st street\n",
      "Fetched video for phrase: deric peralta\n",
      "Prediction: eric peral\n",
      "Fetched video for phrase: www.zobozdrav-vestn.si\n",
      "Prediction: www.mobalder.net\n",
      "Fetched video for phrase: www.nexty-ele.com/6335967\n",
      "Prediction: www.nexty-ele.com/6335967\n",
      "Fetched video for phrase: stars94.bg\n",
      "Prediction: stars94.bg\n",
      "Fetched video for phrase: 1115 paradise meadow\n",
      "Prediction: 151 aradise meadow\n",
      "Fetched video for phrase: malcolm hamilton\n",
      "Prediction: malcolm hamilton\n",
      "Fetched video for phrase: 949-600-2398\n",
      "Prediction: +670-05-98-98-41-89\n",
      "Fetched video for phrase: videoencontexto.com/\n",
      "Prediction: videoenconlexto.com\n"
     ]
    }
   ],
   "source": [
    "video_list = []\n",
    "for sid in train_df['sequence_id'].head(100):\n",
    "    v = get_video_from_sid(sid)\n",
    "    video_list.append(v)\n",
    "    out = prediction_fn(inputs=v)\n",
    "    print('Prediction:', \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(out[REQUIRED_OUTPUT], axis=1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11b7bd87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-25T20:17:52.930909Z",
     "iopub.status.busy": "2023-08-25T20:17:52.930484Z",
     "iopub.status.idle": "2023-08-25T20:18:23.501421Z",
     "shell.execute_reply": "2023-08-25T20:18:23.500042Z"
    },
    "papermill": {
     "duration": 30.602688,
     "end_time": "2023-08-25T20:18:23.515546",
     "exception": false,
     "start_time": "2023-08-25T20:17:52.912858",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check inference time on 100 samples\n",
      "CPU times: user 30.6 s, sys: 14.8 ms, total: 30.6 s\n",
      "Wall time: 30.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('check inference time on 100 samples')\n",
    "for v in video_list:\n",
    "    prediction_fn(inputs=v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 399.657676,
   "end_time": "2023-08-25T20:18:26.454207",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-08-25T20:11:46.796531",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
